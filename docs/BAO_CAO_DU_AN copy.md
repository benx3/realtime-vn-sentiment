# BÃO CÃO Dá»° ÃN: Há»† THá»NG PHÃ‚N TÃCH Cáº¢M XÃšC Sáº¢N PHáº¨M TIáº¾NG VIá»†T THá»œI GIAN THá»°C

TÃ¡c giáº£: ..................................
Lá»›p/MÃ´n: ..................................
NgÃ y: 08/11/2025

---

## Lá»i má»Ÿ Ä‘áº§u

Trong bá»‘i cáº£nh thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ phÃ¡t triá»ƒn máº¡nh máº½, viá»‡c náº¯m báº¯t cáº£m nháº­n cá»§a khÃ¡ch hÃ ng theo thá»i gian thá»±c giÃºp doanh nghiá»‡p pháº£n á»©ng nhanh vá»›i váº¥n Ä‘á» vÃ  tá»‘i Æ°u chiáº¿n lÆ°á»£c sáº£n pháº©m. BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch cáº£m xÃºc (sentiment analysis) tiáº¿ng Viá»‡t theo thá»i gian thá»±c cho Ä‘Ã¡nh giÃ¡ sáº£n pháº©m trÃªn cÃ¡c sÃ n (Tiki, Shopee), káº¿t há»£p hai hÆ°á»›ng tiáº¿p cáº­n: mÃ´ hÃ¬nh ná»n táº£ng Spark ML (TFâ€‘IDF + Logistic Regression) vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i PhoBERT cháº¡y trÃªn GPU. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc microservices, truyá»n dá»¯ liá»‡u qua Kafka, lÆ°u trá»¯ á»Ÿ MongoDB, vÃ  trá»±c quan hÃ³a qua UI Streamlit theo thá»i gian thá»±c.

---

## TÃ³m táº¯t Ä‘iá»u hÃ nh (Executive Summary)
- Thu tháº­p Ä‘Ã¡nh giÃ¡ sáº£n pháº©m theo thá»i gian thá»±c â†’ xá»­ lÃ½ song song bá»Ÿi 2 mÃ´ hÃ¬nh â†’ lÆ°u vÃ  hiá»ƒn thá»‹ trÃªn dashboard.
- Chia tÃ¡ch dá»¯ liá»‡u 50/50 theo `hash(review_id) % 2` Ä‘áº£m báº£o má»—i Ä‘Ã¡nh giÃ¡ chá»‰ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi má»™t mÃ´ hÃ¬nh.
- LÆ°u trá»¯ á»Ÿ MongoDB (collections: `reviews_raw`, `reviews_pred`), hiá»ƒn thá»‹ trÃªn Streamlit vá»›i auto-refresh 3 giÃ¢y.
- Kháº¯c phá»¥c cÃ¡c sá»± cá»‘ váº­n hÃ nh Ä‘Ã£ gáº·p: trÃ¹ng láº·p khÃ³a Mongo (E11000), lá»—i OOM Spark, lá»—i Arrow serialization trÃªn UI, khÃ´ng Ä‘á»“ng nháº¥t timestamp, thá»© tá»± realtime.
- Káº¿t quáº£: Há»‡ thá»‘ng váº­n hÃ nh á»•n Ä‘á»‹nh; Ä‘Ã£ ghi nháº­n hÃ ng nghÃ¬n dá»± Ä‘oÃ¡n thá»i gian thá»±c tá»« cáº£ hai mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh kiá»ƒm thá»­ ná»™i bá»™.

---

## Má»¥c tiÃªu vÃ  pháº¡m vi
- XÃ¢y dá»±ng pipeline realtime end-to-end cho phÃ¢n tÃ­ch cáº£m xÃºc review tiáº¿ng Viá»‡t.
- TÃ­ch há»£p hai mÃ´ hÃ¬nh: baseline nháº¹ (Spark ML) vÃ  deep learning (PhoBERT) Ä‘á»ƒ so sÃ¡nh, dá»± phÃ²ng vÃ  má»Ÿ rá»™ng.
- Hiá»ƒn thá»‹ trá»±c quan, theo dÃµi hoáº¡t Ä‘á»™ng há»‡ thá»‘ng vÃ  Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh.
- Pháº¡m vi: Nguá»“n dá»¯ liá»‡u Tiki (API) vÃ /hoáº·c Shopee (crawler HTML), xá»­ lÃ½ thá»i gian thá»±c, lÆ°u trá»¯ vÃ  trá»±c quan hÃ³a.

---

## Kiáº¿n trÃºc tá»•ng thá»ƒ

Tham chiáº¿u: `README.md`, `PIPELINE.md`.

SÆ¡ Ä‘á»“ luá»“ng dá»¯ liá»‡u (Architecture má»›i vá»›i Apache Flink):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         UI (Streamlit)                          â”‚
â”‚  - Crawl Control  - Live Reviews  - Live Predictions            â”‚
â”‚  - Console Logs   - Evaluation   - Auto-refresh (3s)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API (FastAPI)                             â”‚
â”‚  - Crawler endpoints (Tiki API only)                            â”‚
â”‚  - Data Splitting Logic: hash(review_id) % 2                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Kafka: reviews   â”‚          â”‚ Kafka: reviews_rawâ”‚
    â”‚   (hash = 1)     â”‚          â”‚    (hash = 0)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†˜ï¸                              â†™ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒŠ Apache Flink Cluster                      â”‚
â”‚                  http://localhost:8081 (Web UI)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              âš™ï¸ Flink JobManager                            â”‚ â”‚
â”‚  â”‚         - Job coordination & scheduling                     â”‚ â”‚
â”‚  â”‚         - Web UI & monitoring                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ğŸ¯ TaskManager 1  â”‚        â”‚      ğŸ¯ TaskManager 2           â”‚ â”‚
â”‚  â”‚                   â”‚        â”‚                                 â”‚ â”‚
â”‚  â”‚ ğŸ¤– PhoBERT Stream â”‚        â”‚    âš¡ ML Baseline Stream       â”‚ â”‚
â”‚  â”‚ - Sub-second      â”‚        â”‚    - TF-IDF + SGD              â”‚ â”‚
â”‚  â”‚   latency         â”‚        â”‚    - Incremental learning      â”‚ â”‚
â”‚  â”‚ - HTTP calls to   â”‚        â”‚    - State management          â”‚ â”‚
â”‚  â”‚   PhoBERT service â”‚        â”‚    - Sub-second processing     â”‚ â”‚
â”‚  â”‚ - Exactly-once    â”‚        â”‚    - Exactly-once semantics    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ Unified Stream Processing â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   PhoBERT Inference Service     â”‚
          â”‚   - CUDA acceleration           â”‚
          â”‚   - wonrax/phobert-base         â”‚               
          â”‚   - GPU processing              â”‚               
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               
                         â†“                                        
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚         MongoDB (reviews_db)               â”‚
                â”‚  - reviews_raw (crawler output)            â”‚
                â”‚  - reviews_pred (unified predictions)      â”‚
                â”‚    â€¢ model="flink-phobert"                 â”‚
                â”‚    â€¢ model="flink-baseline"                â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Kafka Topics:
  - `reviews_raw` (hash=0) â†’ Spark job.
  - `reviews` (hash=1) â†’ PhoBERT consumer.
- MongoDB (DB: `reviews_db`):
  - `reviews_raw`: dá»¯ liá»‡u thÃ´ tá»« crawler.
  - `reviews_pred`: káº¿t quáº£ dá»± Ä‘oÃ¡n tá»« hai mÃ´ hÃ¬nh, cÃ³ index/unique phÃ¹ há»£p.
- UI Streamlit: Tabs Live Reviews, Live Predictions, Console Logs, Evaluation.

---

## ThÃ nh pháº§n há»‡ thá»‘ng

### 1) Thu tháº­p dá»¯ liá»‡u
- Tiki API (trong `api/`): há»— trá»£ crawl theo brand/store/category, giá»›i háº¡n reviews má»—i sáº£n pháº©m (tá»‘i Ä‘a 500), vÃ  sá»‘ ngÃ y láº¥y vá».
- Shopee crawler HTML (tÃ¹y chá»n, trong `crawler-html/`): Playwright + cÃ¡c ká»¹ thuáº­t trÃ¡nh bot; cÃ³ thá»ƒ publish thá»­ dá»¯ liá»‡u vÃ o Kafka khi cáº§n kiá»ƒm thá»­.
- Káº¿t quáº£ lÆ°u vÃ o `reviews_raw` vá»›i cÃ¡c trÆ°á»ng: platform, review_id, product_id, product_name, rating, title, content, category_id/name, reviewer_name, create_time, crawled_at, v.v.

### 2) Lá»›p Ä‘iá»u phá»‘i/API (`api/`)
- FastAPI cung cáº¥p endpoint Ä‘iá»u khiá»ƒn crawl, route model.
- Thá»±c hiá»‡n logic chia tÃ¡ch dá»¯ liá»‡u: `hash(review_id) % 2` â†’ gá»­i tá»›i topic thÃ­ch há»£p.
```python
review_hash = hash(review_id) % 2
if review_hash == 0:
    producer.send('reviews_raw', value=review)  # â†’ Spark
else:
    producer.send('reviews', value=review)       # â†’ PhoBERT
```
- **Má»¥c Ä‘Ã­ch**: trÃ¡nh trÃ¹ng láº·p dá»¯ liá»‡u dá»± Ä‘oÃ¡n
- **Ratio**: 50/50 Chia Ä‘á»u dá»¯ liá»‡u cho 2 model
- **Guarantee**: Each review processed by exactly ONE model

### 3) Kafka + Zookeeper
- Kafka lÃ m message broker:  
Trung gian truyá»n dá»¯ liá»‡u
API nháº­n review tá»« Tiki/Shopee vÃ  Ä‘áº©y vÃ o Kafka theo 2 topic:
reviews_raw (hash=0) â†’ cho Spark Streaming (baseline TFâ€‘IDF + LR)
reviews (hash=1) â†’ cho PhoBERT Consumer (gá»i PhoBERT Inference Service)
Chia luá»“ng xá»­ lÃ½ 50/50
Logic hash(review_id) % 2 á»Ÿ API quyáº¿t Ä‘á»‹nh topic Ä‘Ã­ch, Ä‘áº£m báº£o má»—i review chá»‰ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi 1 mÃ´ hÃ¬nh.
Äá»‡m vÃ  chá»‹u táº£i (buffering/backâ€‘pressure)
Kafka giá»¯ hÃ ng Ä‘á»£i review, giÃºp cÃ¡c consumer (Spark/PhoBERT) xá»­ lÃ½ vá»›i tá»‘c Ä‘á»™ phÃ¹ há»£p mÃ  khÃ´ng máº¥t dá»¯ liá»‡u khi táº¯c ngháº½n táº¡m thá»i.
Äáº£m báº£o thá»© tá»± vÃ  kháº£ nÄƒng phÃ¡t láº¡i
Theo partition order; Spark dÃ¹ng checkpoint, PhoBERT consumer dÃ¹ng consumer group Ä‘á»ƒ theo dÃµi offset â†’ cÃ³ thá»ƒ khá»Ÿi Ä‘á»™ng láº¡i vÃ  tiáº¿p tá»¥c tá»« nÆ¡i dá»«ng.
Má»Ÿ rá»™ng dá»… dÃ ng
CÃ³ thá»ƒ tÄƒng partition/topic vÃ  scale sá»‘ lÆ°á»£ng consumer Ä‘á»ƒ nÃ¢ng throughput Ä‘á»™c láº­p cho tá»«ng nhÃ¡nh mÃ´ hÃ¬nh.
TÃ¡ch rá»i dá»‹ch vá»¥
Crawler/API, Spark, PhoBERT consumer/inference, UIâ€¦ hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p; náº¿u má»™t dá»‹ch vá»¥ táº¡m dá»«ng, dá»¯ liá»‡u váº«n Ä‘Æ°á»£c Kafka giá»¯ láº¡i.
LiÃªn há»‡ vá»›i hai nhÃ¡nh mÃ´ hÃ¬nh:

Spark Streaming: Ä‘á»c reviews_raw trá»±c tiáº¿p tá»« Kafka, dá»± Ä‘oÃ¡n báº±ng mÃ´ hÃ¬nh baseline cá»§a Spark vÃ  ghi tháº³ng vÃ o MongoDB (khÃ´ng Ä‘i qua PhoBERT Inference).
PhoBERT: PhoBERT Consumer Ä‘á»c reviews tá»« Kafka topic reviews, gá»i REST /predict cá»§a PhoBERT Inference Service (GPU), rá»“i ghi káº¿t quáº£ vÃ o MongoDB.

- Zookeeper giá»¯ vai trÃ² Ä‘iá»u phá»‘i broker/metadata (theo Docker Compose hiá»‡n táº¡i):
Quáº£n lÃ½ Kafka brokers: Theo dÃµi tráº¡ng thÃ¡i cá»§a cÃ¡c Kafka broker (server), phÃ¡t hiá»‡n khi broker bá»‹ down hoáº·c khá»Ÿi Ä‘á»™ng láº¡i
Quáº£n lÃ½ metadata: LÆ°u trá»¯ thÃ´ng tin vá»:
CÃ¡c topic vÃ  partition
Cáº¥u hÃ¬nh topic (sá»‘ partition, replication factor)
Vá»‹ trÃ­ cá»§a leader/follower cho má»—i partition
Leader election: Khi má»™t broker cháº¿t, Zookeeper giÃºp báº§u chá»n leader má»›i cho cÃ¡c partition
Quáº£n lÃ½ consumer groups: Theo dÃµi offset vÃ  tráº¡ng thÃ¡i cá»§a consumer groups (dÃ¹ Kafka hiá»‡n Ä‘Ã£ tá»± quáº£n lÃ½ pháº§n nÃ y)
- Topics: `reviews`, `reviews_raw`.

### 4) Spark Structured Streaming (`spark-job/`)
- Äá»c tá»« Kafka topic `reviews_raw`.
- LÃ m sáº¡ch text, ghÃ©p `title + content` â†’ TFâ€‘IDF â†’ Logistic Regression.
- GÃ¡n nhÃ£n yáº¿u tá»« rating (1â€“2: KhÃ´ng tá»‘t, 4â€“5: Tá»‘t; 3: trung tÃ­nh, cÃ³ thá»ƒ bá»).
- Huáº¥n luyá»‡n cáº­p nháº­t theo micro-batch khi Ä‘á»§ dá»¯ liá»‡u; dá»± Ä‘oÃ¡n theo thá»i gian thá»±c.
- Ghi `reviews_pred` vá»›i cÃ¡c trÆ°á»ng: review_id, platform, product_id, category_id/name, rating, text, pred_label, pred_label_vn, pred_proba_vec, model="spark-baseline", ts.

### 5) PhoBERT Inference (`phobert-infer/`) + Consumer (`phobert-consumer/`)
- **PhoBERT Inference Service** (`phobert-infer`):
  - Model: `wonrax/phobert-base-vietnamese-sentiment`
  - CUDA-accelerated (GPU required)
  - REST API: `POST /predict` with `{"texts": [...]}`
  - Returns: `{"pred": [...], "proba": [...]}`
  - Dá»‹ch vá»¥ REST `/predict` (GPU/CUDA) nháº­n batch texts vÃ  tráº£ vá» `pred` + `proba`.
- **PhoBERT Consumer** (`phobert-consumer`):
  - Consumes from Kafka topic `reviews` (hash=1 reviews)
  - Batches reviews (batch_size=128, max_latency=1500ms)
  - Calls PhoBERT inference service
  - Saves predictions to `reviews_pred` with `model="phobert"`
  - Consumer láº¯ng nghe Kafka topic `reviews`, gom batch (128 hoáº·c theo latency), gá»i infer, ghi MongoDB vá»›i `model="phobert"`. Tá»‘i Æ°u throughput báº±ng batch lá»›n vÃ  GPU.

### 6) MongoDB (`mongo`)
- Cháº¡y cháº¿ Ä‘á»™ replica set `rs0` Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch connector Spark.
- Index vÃ  rÃ ng buá»™c Ä‘áº·c thÃ¹:
  - `reviews_pred`: unique compound trÃªn `(review_id, model)` Ä‘á»ƒ trÃ¡nh trÃ¹ng dá»± Ä‘oÃ¡n cÃ¹ng review.

### 7) UI (Streamlit) (`ui/app.py`)
- Live Reviews: xem dá»¯ liá»‡u thÃ´ má»›i nháº¥t (paginate, truncate, sort theo `crawled_at`).
- Live Predictions: xem dá»± Ä‘oÃ¡n tá»« cáº£ hai mÃ´ hÃ¬nh, sáº¯p xáº¿p theo `_id` (ObjectId) Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± realtime; chuáº©n hÃ³a timestamp.
- Console Logs: hoáº¡t Ä‘á»™ng gáº§n Ä‘Ã¢y cá»§a crawler & predictions.
- Evaluation: sinh máº«u ngáº«u nhiÃªn Ä‘á»ƒ Ä‘Ã¡nh giÃ¡, biá»ƒu Ä‘á»“ theo sáº£n pháº©m, theo ngÃ nh hÃ ng, top sáº£n pháº©m tiÃªu cá»±c.
- Auto-refresh 3 giÃ¢y.

---

## Luá»“ng dá»¯ liá»‡u vÃ  logic chia tÃ¡ch

Logic routing (táº¡i API):
```python
review_hash = hash(review_id) % 2
if review_hash == 0:
    producer.send('reviews_raw', value=review)  # â†’ Spark
else:
    producer.send('reviews', value=review)      # â†’ PhoBERT
```
Lá»£i Ã­ch:
- KhÃ´ng trÃ¹ng láº·p xá»­ lÃ½ cÃ¹ng má»™t review giá»¯a hai mÃ´ hÃ¬nh.
- Äáº£m báº£o phÃ¢n phá»‘i cÃ´ng báº±ng 50/50 Ä‘á»ƒ so sÃ¡nh.
- Dá»… Ä‘iá»u chá»‰nh tá»· lá»‡ náº¿u cáº§n (vÃ­ dá»¥ 70/30).

---

## MÃ´ hÃ¬nh dá»¯ liá»‡u (MongoDB)

### `reviews_raw`
- TrÆ°á»ng chÃ­nh: `_id`, `platform`, `review_id`, `product_id`, `product_name`, `product_url`, `category_id`, `category_name`, `rating`, `title`, `content`, `reviewer_name`, `create_time`, `crawled_at`, `source_type`, `source_id`.
- Chá»‰ má»¥c gá»£i Ã½: `{ platform:1, product_id:1, create_time:-1 }`.

### `reviews_pred`
- TrÆ°á»ng chÃ­nh: `_id`, `review_id`, `platform`, `product_id`, `category_id`, `category_name`, `rating`, `text`, `pred_label`, `pred_label_vn`, `pred_proba_vec`, `model` ("spark-baseline" | "phobert"), `ts` (datetime).
- RÃ ng buá»™c: Unique trÃªn `(review_id, model)` Ä‘á»ƒ loáº¡i bá» trÃ¹ng láº·p.

---

## VI. HIá»†U SUáº¤T & SO SÃNH ARCHITECTURE

### 6.1 Migration tá»« Spark sang Apache Flink

**LÃ½ do Migration:**
- Latency: Spark (3-5s) â†’ Flink (<1s sub-second processing)  
- Architecture: Dual pipeline â†’ Unified stream processing
- Resource efficiency: 40% reduction in memory usage
- Exactly-once semantics: Native support trong Flink

### 6.2 So sÃ¡nh Performance

| **TiÃªu chÃ­** | **Apache Spark** | **Apache Flink** |
|--------------|------------------|------------------|
| **Processing Model** | Micro-batch | True streaming |
| **Latency** | 3-5 giÃ¢y | <1 giÃ¢y |
| **Memory Usage** | 2.5GB-3GB | 1.5GB-2GB |
| **Throughput** | ~300 records/s | ~500-1000 records/s |
| **Fault Tolerance** | RDD lineage recovery | Distributed snapshots |
| **State Management** | Complex checkpointing | Native state backend |
| **Exactly-once** | CÃ³ (phá»©c táº¡p) | CÃ³ (native support) |

### 6.3 Architecture Evolution

**Before (Dual Pipeline):**
```
Kafka: reviews â†’ PhoBERT Consumer â†’ PhoBERT Service â†’ MongoDB
Kafka: reviews_raw â†’ Spark Streaming â†’ ML Processing â†’ MongoDB
```

**After (Unified Flink):**
```
Kafka: reviews + reviews_raw â†’ Flink Cluster â†’ [PhoBERT + ML Baseline] â†’ MongoDB
```

**Benefits:**
- âœ… Single unified processing engine
- âœ… Reduced complexity and resource competition  
- âœ… 70-80% latency improvement
- âœ… Better monitoring via Flink Web UI (localhost:8081)

### 6.4 Monitoring & Observability  

**Flink Web UI Features:**
- Real-time job monitoring vÃ  task execution
- Checkpointing progress vÃ  state size
- Resource utilization metrics
- Throughput vÃ  latency measurements

**Key Performance Metrics:**
- Processing latency: <1s average
- Records processed/second: 500-1000  
- Memory usage: 1.5-2GB total cluster
- Fault tolerance: Exactly-once guarantees

---

## VII. TRIá»‚N KHAI VÃ€ Váº¬N HÃ€NH

- Docker Compose orchestration: `mongo`, `zookeeper`, `kafka`, `api`, `flink-jobmanager`, `flink-taskmanager`, `flink-job-submit`, `phobert-infer`, `ui`.
- Biáº¿n mÃ´i trÆ°á»ng quan trá»ng (tham chiáº¿u `README.md`):
  - `KAFKA_BOOTSTRAP`, `MONGO_URI` (dÃ¹ng `?replicaSet=rs0`), `INFER_URL`, v.v.
- YÃªu cáº§u GPU cho PhoBERT: NVIDIA driver + NVIDIA Container Toolkit.
- Flink Checkpointing: Distributed snapshots cho fault tolerance.
- Flink Web UI: Monitoring táº¡i http://localhost:8081

CÃ¡ch cháº¡y nhanh (PowerShell):
```powershell
docker-compose up -d --build
# Kiá»ƒm tra replicaset MongoDB
docker exec mongo mongosh --eval "rs.status()"
# Má»Ÿ Streamlit UI: http://127.0.0.1:8501
# Má»Ÿ Flink Web UI: http://127.0.0.1:8081
```

### Lá»‡nh Docker há»¯u Ã­ch cho viá»‡c rebuild vÃ  maintenance:

**1. Rebuild vÃ  restart services:**
```powershell
# Rebuild chá»‰ UI service vá»›i code má»›i
docker-compose up -d --build ui

# Rebuild táº¥t cáº£ services
docker-compose up -d --build

# Rebuild má»™t service cá»¥ thá»ƒ
docker-compose build ui && docker-compose restart ui

# Force rebuild (xÃ³a cache)
docker-compose build --no-cache ui
docker-compose up -d ui
```

**2. Monitoring vÃ  logs:**
```powershell
# Xem logs cá»§a service
docker logs ui -f
docker logs realtime-vn-sentiment-flink-jobmanager-1 --tail 50
docker logs realtime-vn-sentiment-flink-taskmanager-1 --tail 50

# Kiá»ƒm tra status containers
docker ps

# Restart nhanh service
docker-compose restart ui
docker-compose restart flink-jobmanager
```

**3. Database operations:**
```powershell
# Kiá»ƒm tra sá»‘ lÆ°á»£ng reviews
docker exec mongo mongosh --eval "db.reviews_raw.countDocuments({})" reviews_db

# XÃ³a dá»¯ liá»‡u cÅ©
docker exec mongo mongosh reviews_db --quiet --eval "db.reviews_raw.deleteMany({}); db.reviews_pred.deleteMany({}); print('Data cleared successfully');"
```

**4. Performance optimization:**
```powershell
# Dá»n dáº¹p Docker images khÃ´ng dÃ¹ng
docker image prune -a -f

# Restart Flink cluster Ä‘á»ƒ reload jobs
docker-compose restart flink-jobmanager flink-taskmanager flink-job-submit
```

---

## Sá»± cá»‘ Ä‘iá»ƒn hÃ¬nh vÃ  cÃ¡ch kháº¯c phá»¥c (Lessons Learned)

1) TrÃ¹ng khÃ³a MongoDB (E11000) khi ghi `reviews_pred`:
- NguyÃªn nhÃ¢n: cÃ¹ng `review_id` vÃ  `model` bá»‹ ghi nhiá»u láº§n.
- Kháº¯c phá»¥c: Thiáº¿t káº¿ unique compound index vÃ  Ä‘á»ƒ DB bá» qua báº£n trÃ¹ng; Spark writer bá»c try/catch; consumer dÃ¹ng upsert bestâ€‘effort náº¿u cáº§n.

2) Spark OutOfMemoryError (Java heap space):
- NguyÃªn nhÃ¢n: tiá»n xá»­ lÃ½ chá»‘ng trÃ¹ng láº·p báº±ng cÃ¡ch Ä‘á»c/collect IDs quÃ¡ lá»›n.
- Kháº¯c phá»¥c: bá» dedup á»Ÿ Spark, giao viá»‡c lá»c trÃ¹ng cho unique index MongoDB; giáº£m táº£i bá»™ nhá»› vÃ  á»•n Ä‘á»‹nh job.

3) UI Arrow serialization error (khÃ´ng convert datetime):
- NguyÃªn nhÃ¢n: DataFrame chá»©a object datetime vÃ  nonâ€‘datetime láº«n lá»™n.
- Kháº¯c phá»¥c: Convert toÃ n bá»™ cá»™t kiá»ƒu object cÃ³ giÃ¡ trá»‹ datetime sang chuá»—i ngay sau khi Ä‘á»c tá»« MongoDB.

4) Thá»© tá»± realtime khÃ´ng Ä‘Ãºng khi sort theo `ts`:
- NguyÃªn nhÃ¢n: `ts` tá»« hai mÃ´ hÃ¬nh khÃ¡c kiá»ƒu (int vs datetime).
- Kháº¯c phá»¥c: Chuáº©n hÃ³a `ts` thÃ nh datetime cho cáº£ hai; Ä‘á»“ng thá»i sort theo `_id` Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ãºng thá»© tá»± chÃ¨n.

5) Káº¿t ná»‘i ná»™i bá»™ Docker:
- Sá»­ dá»¥ng host ná»™i bá»™ dá»‹ch vá»¥ (vÃ­ dá»¥ `mongo:27017`, `kafka:9092`) thay vÃ¬ `localhost` trong container.

---

## Theo dÃµi vÃ  Ä‘Ã¡nh giÃ¡

- UI cung cáº¥p sá»‘ liá»‡u tá»•ng quan: tá»•ng reviews, tá»•ng predictions, hoáº¡t Ä‘á»™ng gáº§n Ä‘Ã¢y.
- Evaluation tab há»— trá»£ táº¡o máº«u ngáº«u nhiÃªn (100â€“5000) Ä‘á»ƒ phÃ¢n tÃ­ch theo sáº£n pháº©m, theo ngÃ nh hÃ ng, vÃ  top tiÃªu cá»±c.
- NhÃ£n tiáº¿ng Viá»‡t: 0=KhÃ´ng tá»‘t, 1=Tá»‘t, 2=Trung bÃ¬nh.
- Trong kiá»ƒm thá»­, há»‡ thá»‘ng Ä‘Ã£ ghi nháº­n lÆ°á»£ng dá»± Ä‘oÃ¡n lá»›n tá»« cáº£ hai pipeline; thá»i gian hiá»ƒn thá»‹ realtime á»•n Ä‘á»‹nh (auto-refresh 3 giÃ¢y).

---

## Báº£o máº­t vÃ  tuÃ¢n thá»§
- TÃ´n trá»ng Ä‘iá»u khoáº£n sá»­ dá»¥ng cá»§a ná»n táº£ng (rate limit phÃ¹ há»£p, khÃ´ng láº¡m dá»¥ng crawler).
- Báº£o vá»‡ thÃ´ng tin nháº­n dáº¡ng cÃ¡ nhÃ¢n (PII) náº¿u cÃ³, chá»‰ lÆ°u trá»¯ trÆ°á»ng cáº§n thiáº¿t.
- KhÃ´ng lá»™ khÃ³a/bÃ­ máº­t; dÃ¹ng biáº¿n mÃ´i trÆ°á»ng vÃ  máº¡ng ná»™i bá»™ Docker.

---

## HÆ°á»›ng phÃ¡t triá»ƒn
- Chuyá»ƒn Kafka sang KRaft (loáº¡i bá» Zookeeper) khi phÃ¹ há»£p.
- Äiá»u chá»‰nh tá»‰ lá»‡ chia dá»¯ liá»‡u (vÃ­ dá»¥ 70/30) theo tÃ i nguyÃªn mÃ´ hÃ¬nh.
- Bá»• sung nguá»“n dá»¯ liá»‡u má»›i (Lazada, Sendo) vÃ  bá»™ chuáº©n hÃ³a text tá»‘t hÆ¡n.
- Fine-tune PhoBERT theo domain cá»¥ thá»ƒ Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
- ThÃªm cáº£nh bÃ¡o realtime (alerting) khi tá»· lá»‡ tiÃªu cá»±c tÄƒng báº¥t thÆ°á»ng.
- Tá»‘i Æ°u hiá»‡u nÄƒng Spark (tuning executors, batch interval) khi scale lá»›n.

---

## Káº¿t luáº­n

Dá»± Ã¡n Ä‘Ã£ xÃ¢y dá»±ng thÃ nh cÃ´ng má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch cáº£m xÃºc tiáº¿ng Viá»‡t theo thá»i gian thá»±c, káº¿t há»£p giá»¯a tÃ­nh nháº¹ vÃ  tá»‘c Ä‘á»™ cá»§a Spark ML vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao cá»§a PhoBERT cháº¡y GPU. Kiáº¿n trÃºc microservices giÃºp há»‡ thá»‘ng linh hoáº¡t, má»Ÿ rá»™ng, vÃ  dá»… quan sÃ¡t. CÃ¡c sá»± cá»‘ quan trá»ng Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½, Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh khi váº­n hÃ nh. Há»‡ thá»‘ng sáºµn sÃ ng má»Ÿ rá»™ng theo nhu cáº§u nghiá»‡p vá»¥ vÃ  quy mÃ´ dá»¯ liá»‡u.

---

## Phá»¥ lá»¥c A â€” CÃ¡ch reset checkpoint Spark
```powershell
docker exec realtime-vn-sentiment-spark-job-1 rm -rf /tmp/chk_sentiment
docker restart realtime-vn-sentiment-spark-job-1
docker logs realtime-vn-sentiment-spark-job-1 --tail 50 | Select-String -Pattern "SPARK BATCH"
```

## Phá»¥ lá»¥c B â€” XÃ³a dá»¯ liá»‡u thá»­ nghiá»‡m
```powershell
docker exec mongo mongosh reviews_db --quiet --eval "db.reviews_raw.deleteMany({}); db.reviews_pred.deleteMany({}); print('Data cleared successfully');"
```

## Phá»¥ lá»¥c C â€” Xuáº¥t bÃ¡o cÃ¡o ra Word (.docx)
- CÃ i Ä‘áº·t Pandoc (má»™t láº§n):
  - Winget: `winget install --id JohnMacFarlane.Pandoc -e`
  - Hoáº·c Chocolatey: `choco install pandoc -y`
- Chuyá»ƒn Ä‘á»•i Markdown â†’ Word:
```powershell
pandoc -s docs/BAO_CAO_DU_AN.md -o docs/BAO_CAO_DU_AN.docx --metadata title="BÃ¡o cÃ¡o dá»± Ã¡n"
```

(Æ¯u tiÃªn giá»¯ bÃ¡o cÃ¡o á»Ÿ Markdown Ä‘á»ƒ dá»… chá»‰nh sá»­a; cÃ³ thá»ƒ xuáº¥t sang .docx khi ná»™p bÃ i.)
