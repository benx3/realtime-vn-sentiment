import streamlit as st, pandas as pd, time, requests, os
from pymongo import MongoClient
import plotly.express as px
import math

API_BASE = os.getenv("API_BASE","http://api:8000")
MONGO_URI = os.getenv("MONGO_URI","mongodb://mongo:27017")
INFER_URL = os.getenv("INFER_URL","http://phobert-infer:5000")

st.set_page_config(page_title="VN Sentiment Realtime", layout="wide")
st.title("üìä VN Product Sentiment ‚Äî Realtime Control")

# Auto-refresh configuration
REFRESH_INTERVAL = 3

# Initialize refresh counter
if 'refresh_counter' not in st.session_state:
    st.session_state.refresh_counter = 0

# Session state for pagination
if 'reviews_page' not in st.session_state:
    st.session_state.reviews_page = 1
if 'pred_page' not in st.session_state:
    st.session_state.pred_page = 1
if 'reviews_per_page' not in st.session_state:
    st.session_state.reviews_per_page = 20
if 'pred_per_page' not in st.session_state:
    st.session_state.pred_per_page = 20

try:
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
    client.server_info()  # Force connection
    db = client["reviews_db"]  # Use reviews_db to match Spark job
except Exception as e:
    st.error(f"Cannot connect to MongoDB: {e}")
    st.stop()

with st.sidebar:
    st.subheader("Tiki Crawler")
    st.write("Support URLs:")
    st.write("‚Ä¢ Brand: `tiki.vn/thuong-hieu/brand-name.html`")
    st.write("‚Ä¢ Store: `tiki.vn/cua-hang/store-name`") 
    st.write("‚Ä¢ Category: `tiki.vn/category-name/c1234`")
    
    tiki_urls = st.text_area("Tiki URLs (one per line)", 
                             "https://tiki.vn/do-choi-me-be/c2549")
    col1, col2 = st.columns(2)
    with col1:
        max_products_tiki = st.number_input("Max products", 10, 500, 50, 10, key="tiki_products")
        max_reviews_tiki = st.number_input("Reviews per product", 5, 500, 100, 5, key="tiki_reviews", help="Can now get up to 500 reviews per product")
    with col2:
        days_back_tiki = st.number_input("Days back", 1, 365, 30, 1, key="tiki_days")
    
    c_tiki = st.columns(3)
    if c_tiki[0].button("Start Tiki Crawl"):
        urls = [u.strip() for u in tiki_urls.splitlines() if u.strip()]
        if urls:
            payload = {
                "urls": urls,
                "max_products": int(max_products_tiki),
                "max_reviews_per_product": int(max_reviews_tiki),
                "days_back": int(days_back_tiki)
            }
            try:
                r = requests.post(f"{API_BASE}/crawl/tiki/start", json=payload)
                st.success(r.json())
            except Exception as e:
                st.error(f"Error: {e}")
        else:
            st.warning("Please enter at least one Tiki URL")
    
    if c_tiki[1].button("Stop Tiki"):
        r = requests.post(f"{API_BASE}/crawl/tiki/stop")
        st.warning(r.json())
    
    if c_tiki[2].button("Tiki Status"):
        try:
            r = requests.get(f"{API_BASE}/crawl/tiki/status")
            st.json(r.json())
        except Exception as e:
            st.error(f"Error: {e}")

    st.subheader("Model routing")
    active_model = st.selectbox("Active model", ["spark-baseline","phobert","both"], index=2)
    if st.button("Apply Global Routing"):
        requests.post(f"{API_BASE}/control/model", json={"active_model":active_model})
        st.success({"ok": True, "active_model": active_model})
    
    st.divider()
    st.subheader("üóëÔ∏è Data Management")
    
    # Auto-refresh toggle
    auto_refresh = st.checkbox("‚è±Ô∏è Auto-refresh (3s)", value=True, help="Automatically refresh dashboard every 3 seconds")
    
    st.warning("‚ö†Ô∏è Danger Zone: This will permanently delete data!")
    col_clean1, col_clean2 = st.columns(2)
    with col_clean1:
        if st.button("üßπ Clear Reviews", type="secondary", use_container_width=True):
            result = db.reviews_raw.delete_many({})
            st.success(f"‚úÖ Deleted {result.deleted_count:,} reviews")
            time.sleep(1)
            st.rerun()
    with col_clean2:
        if st.button("üßπ Clear Predictions", type="secondary", use_container_width=True):
            result = db.reviews_pred.delete_many({})
            st.success(f"‚úÖ Deleted {result.deleted_count:,} predictions")
            time.sleep(1)
            st.rerun()
    if st.button("üóëÔ∏è Clear All Data", type="primary", use_container_width=True):
        raw_result = db.reviews_raw.delete_many({})
        pred_result = db.reviews_pred.delete_many({})
        st.success(f"‚úÖ Deleted {raw_result.deleted_count:,} reviews + {pred_result.deleted_count:,} predictions")
        time.sleep(1)
        st.rerun()

live_tab, pred_tab, console_tab, eval_tab = st.tabs(["üü¢ Live Reviews", "üîÆ Live Predictions", "üñ•Ô∏è Console Logs", "üìà Evaluation"]) 

with live_tab:
    # Controls for reviews
    col1, col2, col3 = st.columns([2, 2, 2])
    with col1:
        reviews_per_page = st.selectbox("Rows per page", [10, 20, 50, 100], 
                                        index=[10, 20, 50, 100].index(st.session_state.reviews_per_page),
                                        key="reviews_rows_select")
        st.session_state.reviews_per_page = reviews_per_page
    with col2:
        reviews_page_input = st.number_input("Page", min_value=1, value=st.session_state.reviews_page, 
                                            step=1, key="reviews_page_input")
        if reviews_page_input != st.session_state.reviews_page:
            st.session_state.reviews_page = reviews_page_input
            st.rerun()
    with col3:
        pass  # Column not used for now
        
    # Live Reviews Data Processing
    total_reviews = db.reviews_raw.count_documents({})
    skip_reviews = (st.session_state.reviews_page - 1) * st.session_state.reviews_per_page
    raw = list(db.reviews_raw.find()
              .sort([("crawled_at", -1)])  # Newest first by crawl time
              .skip(skip_reviews)
              .limit(st.session_state.reviews_per_page))
    
    # Process and display reviews data
    if raw:
        df_raw = pd.DataFrame(raw)
        
        # Convert datetime columns to string immediately to avoid Arrow serialization errors
        for col in df_raw.columns:
            if df_raw[col].dtype == 'object':
                df_raw[col] = df_raw[col].apply(
                    lambda v: v.strftime("%Y-%m-%d %H:%M:%S") if hasattr(v, 'strftime') else (str(v) if pd.notna(v) else "")
                )
        
        if "platform" in df_raw.columns:
            display_cols = ["platform", "category_name", "reviewer_name", "product_name", "content", "create_time"]
            available_cols = [col for col in display_cols if col in df_raw.columns]
            
            # Select columns first to maintain order from MongoDB
            df_raw = df_raw[available_cols].fillna("")
            
            # Reset index to ensure proper display order
            df_raw = df_raw.reset_index(drop=True)
            
            # Truncate long product names for better display
            if "product_name" in df_raw.columns:
                df_raw['product_name'] = df_raw['product_name'].apply(
                    lambda x: (str(x)[:40] + "...") if pd.notna(x) and len(str(x)) > 40 else str(x)
                )
            
            # Truncate long content for better display
            if "content" in df_raw.columns:
                df_raw['content'] = df_raw['content'].apply(
                    lambda x: (str(x)[:80] + "...") if pd.notna(x) and len(str(x)) > 80 else str(x)
                )
        
        # Display reviews info and table
        total_pages_reviews = max(1, math.ceil(total_reviews / st.session_state.reviews_per_page))
        st.info(f"üìä Showing {len(df_raw)} reviews | Page {st.session_state.reviews_page}/{total_pages_reviews} | Total: {total_reviews:,}")
        st.dataframe(df_raw, use_container_width=True, height=400)
    else:
        st.warning("No reviews data available")

with pred_tab:
    # Controls for predictions
    col1, col2, col3 = st.columns([2, 2, 2])
    with col1:
        pred_per_page = st.selectbox("Rows per page", [10, 20, 50, 100], 
                                     index=[10, 20, 50, 100].index(st.session_state.pred_per_page),
                                     key="pred_rows_select")
        st.session_state.pred_per_page = pred_per_page
    with col2:
        pred_page_input = st.number_input("Page", min_value=1, value=st.session_state.pred_page, 
                                         step=1, key="pred_page_input")
        if pred_page_input != st.session_state.pred_page:
            st.session_state.pred_page = pred_page_input
            st.rerun()
    with col3:
        pass  # Column not used for now
        
    # Live Predictions Data Processing
    total_predictions = db.reviews_pred.count_documents({})
    skip_pred = (st.session_state.pred_page - 1) * st.session_state.pred_per_page
    # Use _id (ObjectId) for reliable descending insertion order across mixed ts types (int vs datetime)
    pred = list(db.reviews_pred.find()
               .sort([("_id", -1)])
               .skip(skip_pred)
               .limit(st.session_state.pred_per_page))
    
    # Process predictions data
    if pred:
        df_pred = pd.DataFrame(pred)
        
        # Check if text/content field exists in predictions
        content_col = None
        if "text" in df_pred.columns:
            content_col = "text"
        elif "content" in df_pred.columns:
            content_col = "content"
        elif "product_name" in df_pred.columns:
            content_col = "product_name"  # Fallback to product name
        
        # Truncate long text for display
        if content_col:
            df_pred['content_display'] = df_pred[content_col].apply(
                lambda x: (str(x)[:100] + "...") if pd.notna(x) and len(str(x)) > 100 else str(x)
            )
        else:
            df_pred['content_display'] = "N/A"
        
        # Ensure reviewer_name exists, fallback to Anonymous if missing
        if 'reviewer_name' not in df_pred.columns:
            df_pred['reviewer_name'] = 'Anonymous'
        else:
            # Fill missing values
            df_pred['reviewer_name'] = df_pred['reviewer_name'].fillna('Anonymous')
        
        # Convert datetime columns to string immediately to avoid Arrow serialization errors
        for col in df_pred.columns:
            if df_pred[col].dtype == 'object':
                # Apply conversion to all values in the column
                df_pred[col] = df_pred[col].apply(
                    lambda v: v.strftime("%Y-%m-%d %H:%M:%S") if hasattr(v, 'strftime') else (str(v) if pd.notna(v) else "")
                )
        
        # Map sentiment labels to Vietnamese
        if "sentiment_label" in df_pred.columns:
            def translate_sentiment(label):
                translations = {
                    'positive': 'T√≠ch c·ª±c',
                    'negative': 'Ti√™u c·ª±c', 
                    'neutral': 'Trung t√≠nh'
                }
                return translations.get(str(label).lower(), str(label))
            
            df_pred['pred_label_vn'] = df_pred['sentiment_label'].apply(translate_sentiment)
        
        # Use timestamp if ts doesn't exist
        if "timestamp" in df_pred.columns and "ts" not in df_pred.columns:
            df_pred['ts'] = df_pred['timestamp']
        
        # Check if pred_label_vn exists, otherwise fallback
        if "pred_label_vn" in df_pred.columns:
            display_cols = ["platform", "category_name", "reviewer_name", "product_id", "review_id", "pred_label_vn", "content_display", "model", "ts"]
        elif "sentiment_label" in df_pred.columns:
            display_cols = ["platform", "category_name", "reviewer_name", "product_id", "review_id", "sentiment_label", "content_display", "model", "ts"]
        else:
            display_cols = ["platform", "category_name", "reviewer_name", "product_id", "review_id", "pred_label", "content_display", "model", "ts"]
        
        available_cols = [col for col in display_cols if col in df_pred.columns]
        df_pred_display = df_pred[available_cols].fillna("")
        
        # Reset index to maintain order from MongoDB sort
        df_pred_display = df_pred_display.reset_index(drop=True)
        
        # Rename column for better display
        df_pred_display = df_pred_display.rename(columns={"content_display": "content", "ts": "timestamp"})

        # Display predictions info and table
        total_pages_pred = max(1, math.ceil(total_predictions / st.session_state.pred_per_page))
        st.info(f"üîÆ Showing {len(df_pred_display)} predictions | Page {st.session_state.pred_page}/{total_pages_pred} | Total: {total_predictions:,} | Last refresh: {time.strftime('%H:%M:%S')} ")
        st.dataframe(df_pred_display, use_container_width=True, height=400)
    else:
        st.warning("No predictions data available")

with console_tab:
    st.subheader("üñ•Ô∏è System Console Logs")
    
    # Real-time system stats
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üì• Total Reviews", f"{db.reviews_raw.count_documents({}):,}")
    with col2:
        st.metric("üîÆ Total Predictions", f"{db.reviews_pred.count_documents({}):,}")
    with col3:
        # Get recent activity (last 5 minutes)
        from datetime import datetime, timedelta
        five_min_ago = datetime.now() - timedelta(minutes=5)
        recent_count = db.reviews_raw.count_documents({"crawled_at": {"$gte": five_min_ago}})
        st.metric("üöÄ Recent Activity (5min)", f"{recent_count}")
    
    # Recent crawler activity
    st.subheader("üì° Recent Crawler Activity")
    recent_reviews = list(db.reviews_raw.find()
                         .sort([("crawled_at", -1)])
                         .limit(10))
    
    if recent_reviews:
        log_data = []
        for review in recent_reviews:
            product_name = review.get("product_name", "N/A")
            if product_name and len(product_name) > 50:
                product_display = product_name[:50] + "..."
            else:
                product_display = product_name or "N/A"
                
            log_data.append({
                "Timestamp": review.get("crawled_at", "N/A"),
                "Platform": review.get("platform", "N/A"),
                "Category": review.get("category_name", "N/A"),
                "Product": product_display,
                "Reviewer": review.get("reviewer_name", "Anonymous"),
                "Rating": f"‚≠ê {review.get('rating', 'N/A')}"
            })
        st.dataframe(pd.DataFrame(log_data), use_container_width=True)
    else:
        st.info("üîÑ Waiting for crawler activity...")
    
    # Recent prediction activity  
    st.subheader("üîÆ Recent Prediction Activity")
    # Sort by _id (ObjectId insertion order) for true real-time ordering
    recent_predictions = list(db.reviews_pred.find()
                             .sort([("_id", -1)])
                             .limit(10))
    
    if recent_predictions:
        pred_log_data = []
        for pred in recent_predictions:
            # Get review details - use direct fields from prediction first, fallback to join
            review_id = pred.get("review_id")
            category_name = pred.get("category_name", "N/A")
            product_id = pred.get("product_id", "N/A")
            
            # Convert timestamp to string if it's a datetime object
            ts_value = pred.get("ts", "N/A")
            if hasattr(ts_value, 'strftime'):
                ts_display = ts_value.strftime("%Y-%m-%d %H:%M:%S")
            else:
                ts_display = str(ts_value) if ts_value else "N/A"
            
            # Try to get product name from raw reviews
            review_detail = db.reviews_raw.find_one({"review_id": review_id}) if review_id else None
            if review_detail:
                product_name = review_detail.get("product_name", "N/A")
                if product_name and len(product_name) > 40:
                    product_display = product_name[:40] + "..."
                else:
                    product_display = product_name or "N/A"
                # Update category from raw if available
                if review_detail.get("category_name"):
                    category_name = review_detail.get("category_name")
            else:
                # No raw review found, use product_id as display
                product_display = f"Product {product_id}"
                
            pred_log_data.append({
                "Timestamp": ts_display,
                "Review ID": str(review_id)[:12] + "..." if review_id else "N/A",
                "Category": category_name,
                "Product": product_display,
                "Prediction": pred.get("pred_label_vn", pred.get("pred", "N/A")),
                "Model": pred.get("model", "N/A")
            })
        st.dataframe(pd.DataFrame(pred_log_data), use_container_width=True)
    else:
        st.info("üîÑ Waiting for prediction activity...")

with eval_tab:
    st.subheader("Manual Evaluation")
    
    # Initialize session state
    if 'eval_last_df' not in st.session_state:
        st.session_state.eval_last_df = None
    if 'eval_last_error' not in st.session_state:
        st.session_state.eval_last_error = None
    if 'eval_last_info' not in st.session_state:
        st.session_state.eval_last_info = None
    
    # Sampling controls
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        k = st.number_input("Sample size", 100, 2000, 500, 100, help="Number of reviews to sample randomly (max 2000 for performance)")
    with col2:
        st.write("")  # Spacer
        st.write("")  # Spacer
        generate_click = st.button("üé≤ Generate Sample", type="primary", use_container_width=True)
    with col3:
        st.write("")  # Spacer
        st.write("")  # Spacer
        if st.button("üóëÔ∏è Clear Results", type="secondary", use_container_width=True):
            st.session_state.eval_last_df = None
            st.session_state.eval_last_error = None
            st.session_state.eval_last_info = "Results cleared. Auto-refresh resumed."
            st.rerun()
    
    # Performance warning
    if k > 1000:
        st.warning(f"‚ö†Ô∏è Large sample size ({k}) may take longer to process. Consider using smaller samples for faster results.")
    
    if generate_click:
        try:
            cur = db.reviews_raw.aggregate([{ "$sample": { "size": int(k) } }])
            rows = list(cur)
        except Exception as e:
            st.session_state.eval_last_error = f"Mongo aggregation failed: {e}"
            st.error(st.session_state.eval_last_error)
            rows = []
        if not rows:
            st.session_state.eval_last_info = "No reviews available in database. Please crawl some data first."
            st.warning(st.session_state.eval_last_info)
        else:
            texts = [f"{r.get('title','')} {r.get('content','')}".strip() for r in rows]
            valid_data = [(r, t) for r, t in zip(rows, texts) if len(t) > 3]
            if not valid_data:
                st.session_state.eval_last_error = "No valid text data found in sampled reviews."
                st.error(st.session_state.eval_last_error)
            else:
                rows_filtered, texts_filtered = zip(*valid_data)
                try:
                    # Process in smaller batches to avoid timeout
                    batch_size = 100
                    all_preds = []
                    texts_list = list(texts_filtered)
                    
                    # Show progress bar
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    success = True
                    total_batches = (len(texts_list) + batch_size - 1) // batch_size
                    
                    for i in range(0, len(texts_list), batch_size):
                        batch_texts = texts_list[i:i + batch_size]
                        current_batch = i // batch_size + 1
                        progress = (current_batch) / total_batches
                        progress_bar.progress(progress)
                        status_text.text(f"Processing batch {current_batch}/{total_batches} ({len(batch_texts)} texts)...")
                        
                        try:
                            resp = requests.post(f"{INFER_URL}/predict", json={"texts": batch_texts}, timeout=120)
                            if resp.status_code != 200:
                                st.session_state.eval_last_error = f"Inference HTTP {resp.status_code}: {resp.text[:200]}"
                                success = False
                                break
                            else:
                                res = resp.json()
                                batch_preds = res.get("pred")
                                if batch_preds:
                                    all_preds.extend(batch_preds)
                                else:
                                    st.session_state.eval_last_error = f"Batch {current_batch} returned no predictions"
                                    success = False
                                    break
                        except Exception as e:
                            st.session_state.eval_last_error = f"Error in batch {current_batch}: {str(e)}"
                            success = False
                            break
                    
                    # Clear progress indicators
                    progress_bar.empty()
                    status_text.empty()
                    
                    if not success:
                        st.error(st.session_state.eval_last_error)
                        st.session_state.eval_last_df = None  # Clear previous results
                    elif not all_preds:
                        st.session_state.eval_last_error = "No predictions received from inference service."
                        st.error(st.session_state.eval_last_error)
                        st.session_state.eval_last_df = None  # Clear previous results
                    elif len(all_preds) != len(texts_list):
                        st.session_state.eval_last_error = f"Mismatch: Expected {len(texts_list)} predictions, got {len(all_preds)}"
                        st.error(st.session_state.eval_last_error)
                        st.session_state.eval_last_df = None  # Clear previous results
                    else:
                        preds = all_preds
                        min_len = min(len(rows_filtered), len(preds))
                        def label_to_vietnamese(pred_label):
                            label_map = {0: "Kh√¥ng t·ªët", 1: "T·ªët", 2: "Trung b√¨nh"}
                            return label_map.get(pred_label, f"Unknown({pred_label})")
                        
                        # Build dataframe with all necessary fields
                        product_names = []
                        product_names_full = []
                        product_ids = []
                        review_ids = []
                        
                        for r in rows_filtered[:min_len]:
                            # Store full product name
                            full_name = r.get("product_name", r.get("product_id", "Unknown"))
                            product_names_full.append(full_name)
                            
                            # Create display name (truncated)
                            pname = full_name
                            if pname and len(pname) > 40:
                                pname = pname[:40] + "..."
                            product_names.append(pname)
                            
                            # Store product_id and review_id for queries
                            product_ids.append(r.get("product_id", ""))
                            review_ids.append(r.get("review_id", ""))
                        
                        df = pd.DataFrame({
                            "review_id": review_ids,
                            "product_id": product_ids,
                            "product_name": product_names,
                            "product_name_full": product_names_full,
                            "category_name": [r.get("category_name", "Unknown") for r in rows_filtered[:min_len]],
                            "pred_label": [label_to_vietnamese(p) for p in preds[:min_len]],
                            "pred_numeric": preds[:min_len]
                        })
                        st.session_state.eval_last_df = df
                        st.session_state.eval_last_error = None
                        st.session_state.eval_last_info = f"Sampled {min_len} reviews successfully processed."
                except Exception as e:
                    st.session_state.eval_last_error = f"Error calling inference service: {e}"
                    st.error(st.session_state.eval_last_error)
    
    # Render previous successful sample if exists and no new error
    if st.session_state.eval_last_df is not None and st.session_state.eval_last_error is None:
        df = st.session_state.eval_last_df
        st.info(st.session_state.eval_last_info or "Sample ready")
        
        # Store full review data for detail view
        if 'eval_full_reviews' not in st.session_state:
            st.session_state.eval_full_reviews = {}
        
        c1, c2 = st.columns(2)
        with c1:
            st.subheader("Top 10 Products by Reviews")
            top_products = df['product_name'].value_counts().head(10).index.tolist()
            df_top = df[df['product_name'].isin(top_products)]
            fig_prod = px.histogram(df_top, y="product_name", color="pred_label",
                                   labels={"pred_label": "ƒê√°nh gi√°", "count": "S·ªë l∆∞·ª£ng"},
                                   color_discrete_map={"T·ªët": "#2ecc71", "Trung b√¨nh": "#f39c12", "Kh√¥ng t·ªët": "#e74c3c"},
                                   category_orders={"pred_label": ["T·ªët", "Trung b√¨nh", "Kh√¥ng t·ªët"]})
            fig_prod.update_layout(
                yaxis={'categoryorder':'total ascending'},
                height=400,
                margin=dict(l=20, r=20, t=20, b=20)
            )
            st.plotly_chart(fig_prod, use_container_width=True, config={'displayModeBar': False})
        with c2:
            st.subheader("By category")
            fig_cat = px.histogram(df, x="category_name", color="pred_label",
                                  labels={"pred_label": "ƒê√°nh gi√°", "count": "S·ªë l∆∞·ª£ng"},
                                  color_discrete_map={"T·ªët": "#2ecc71", "Trung b√¨nh": "#f39c12", "Kh√¥ng t·ªët": "#e74c3c"},
                                  category_orders={"pred_label": ["T·ªët", "Trung b√¨nh", "Kh√¥ng t·ªët"]})
            fig_cat.update_layout(
                height=400,
                margin=dict(l=20, r=20, t=20, b=20),
                xaxis_tickangle=-45
            )
            st.plotly_chart(fig_cat, use_container_width=True, config={'displayModeBar': False})
        
        # Top 10 Worst Products (most negative reviews)
        st.divider()
        st.subheader("üîª Top 10 Worst Products (Most Negative Reviews)")
        
        # Count negative reviews per product
        df_negative = df[df['pred_label'] == 'Kh√¥ng t·ªët']
        if len(df_negative) > 0:
            negative_counts = df_negative['product_name'].value_counts().head(10)
            worst_products = negative_counts.index.tolist()
            
            # Create bar chart for worst products
            df_worst = pd.DataFrame({
                'product_name': negative_counts.index,
                'negative_count': negative_counts.values
            })
            
            fig_worst = px.bar(df_worst, x='negative_count', y='product_name',
                             orientation='h',
                             labels={'negative_count': 'S·ªë ƒë√°nh gi√° ti√™u c·ª±c', 'product_name': 'S·∫£n ph·∫©m'},
                             color='negative_count',
                             color_continuous_scale=['#ffcccc', '#ff0000'])
            fig_worst.update_layout(
                yaxis={'categoryorder':'total ascending'}, 
                showlegend=False,
                height=400,
                margin=dict(l=20, r=20, t=20, b=20)
            )
            st.plotly_chart(fig_worst, use_container_width=True, config={'displayModeBar': False})
            
            # Product selector for detailed reviews
            st.divider()
            st.subheader("üìù Review Details")
            
            # Search options
            search_type = st.radio("Search by:", ["Product Name", "Product ID"], horizontal=True)
            
            product_reviews = None
            
            if search_type == "Product Name":
                # Dropdown selector for product names (using full names for matching)
                product_names_unique = sorted(df['product_name_full'].unique().tolist())
                # Create display options (truncate if needed)
                display_options = ['-- Choose a product --']
                for pname in product_names_unique:
                    display_name = pname if len(pname) <= 50 else pname[:50] + "..."
                    display_options.append(display_name)
                
                selected_display = st.selectbox(
                    "Select product:",
                    options=display_options,
                    key='product_selector'
                )
                
                if selected_display and selected_display != '-- Choose a product --':
                    # Find the matching full product name
                    idx = display_options.index(selected_display) - 1  # -1 for the "Choose" option
                    selected_product_full = product_names_unique[idx]
                    
                    # Filter reviews for selected product using full name
                    product_reviews = df[df['product_name_full'] == selected_product_full]
                    st.caption(f"**Product:** {selected_product_full}")
            else:
                # Text input for product ID or partial name search
                search_query = st.text_input("Enter Product ID or partial Product Name:", key='product_search')
                
                if search_query:
                    # Try exact product_id match first
                    product_reviews = df[df['product_id'] == search_query]
                    
                    if len(product_reviews) == 0:
                        # Fallback to partial product_name_full match
                        product_reviews = df[df['product_name_full'].str.contains(search_query, case=False, na=False)]
                        
                    if len(product_reviews) == 0:
                        st.warning(f"No products found matching: {search_query}")
                        product_reviews = None
                    else:
                        # Show matched product info
                        matched_products = product_reviews['product_name_full'].unique()
                        st.caption(f"**Found {len(matched_products)} product(s):** {', '.join(matched_products[:3])}" + 
                                 (f" and {len(matched_products)-3} more..." if len(matched_products) > 3 else ""))
            
            # Display review details if a product is selected
            if product_reviews is not None and len(product_reviews) > 0:
                
                # Summary stats
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    total = len(product_reviews)
                    st.metric("Total Reviews", total)
                with col2:
                    good = len(product_reviews[product_reviews['pred_label'] == 'T·ªët'])
                    st.metric("T·ªët", good, delta=f"{good/total*100:.1f}%" if total > 0 else "0%")
                with col3:
                    neutral = len(product_reviews[product_reviews['pred_label'] == 'Trung b√¨nh'])
                    st.metric("Trung b√¨nh", neutral, delta=f"{neutral/total*100:.1f}%" if total > 0 else "0%")
                with col4:
                    bad = len(product_reviews[product_reviews['pred_label'] == 'Kh√¥ng t·ªët'])
                    st.metric("Kh√¥ng t·ªët", bad, delta=f"{bad/total*100:.1f}%" if total > 0 else "0%")
                
                # Show reviews in tabs by sentiment
                tab_good, tab_neutral, tab_bad = st.tabs(["‚úÖ T·ªët", "‚ö†Ô∏è Trung b√¨nh", "‚ùå Kh√¥ng t·ªët"])
                
                with tab_good:
                    good_reviews = product_reviews[product_reviews['pred_label'] == 'T·ªët']
                    if len(good_reviews) > 0:
                        st.info(f"Showing {len(good_reviews)} positive reviews")
                        # Get full review content from MongoDB using review_id
                        for idx, row in good_reviews.iterrows():
                            review_doc = db.reviews_raw.find_one({"review_id": row['review_id']})
                            if review_doc:
                                with st.expander(f"‚≠ê {review_doc.get('rating', 'N/A')} - {review_doc.get('reviewer_name', 'Anonymous')}"):
                                    st.write(f"**Title:** {review_doc.get('title', 'N/A')}")
                                    st.write(f"**Content:** {review_doc.get('content', 'N/A')}")
                                    st.caption(f"Product ID: {row['product_id']} | Category: {row['category_name']} | Created: {review_doc.get('create_time', 'N/A')}")
                            else:
                                st.warning(f"Review {row['review_id']} not found in database")
                    else:
                        st.info("No positive reviews found")
                
                with tab_neutral:
                    neutral_reviews = product_reviews[product_reviews['pred_label'] == 'Trung b√¨nh']
                    if len(neutral_reviews) > 0:
                        st.info(f"Showing {len(neutral_reviews)} neutral reviews")
                        for idx, row in neutral_reviews.iterrows():
                            review_doc = db.reviews_raw.find_one({"review_id": row['review_id']})
                            if review_doc:
                                with st.expander(f"‚≠ê {review_doc.get('rating', 'N/A')} - {review_doc.get('reviewer_name', 'Anonymous')}"):
                                    st.write(f"**Title:** {review_doc.get('title', 'N/A')}")
                                    st.write(f"**Content:** {review_doc.get('content', 'N/A')}")
                                    st.caption(f"Product ID: {row['product_id']} | Category: {row['category_name']} | Created: {review_doc.get('create_time', 'N/A')}")
                            else:
                                st.warning(f"Review {row['review_id']} not found in database")
                    else:
                        st.info("No neutral reviews found")
                
                with tab_bad:
                    bad_reviews = product_reviews[product_reviews['pred_label'] == 'Kh√¥ng t·ªët']
                    if len(bad_reviews) > 0:
                        st.info(f"Showing {len(bad_reviews)} negative reviews")
                        for idx, row in bad_reviews.iterrows():
                            review_doc = db.reviews_raw.find_one({"review_id": row['review_id']})
                            if review_doc:
                                with st.expander(f"‚≠ê {review_doc.get('rating', 'N/A')} - {review_doc.get('reviewer_name', 'Anonymous')}"):
                                    st.write(f"**Title:** {review_doc.get('title', 'N/A')}")
                                    st.write(f"**Content:** {review_doc.get('content', 'N/A')}")
                                    st.caption(f"Product ID: {row['product_id']} | Category: {row['category_name']} | Created: {review_doc.get('create_time', 'N/A')}")
                            else:
                                st.warning(f"Review {row['review_id']} not found in database")
                    else:
                        st.info("No negative reviews found")
        else:
            st.warning("No negative reviews found in the sample")
    elif st.session_state.eval_last_error:
        st.error(st.session_state.eval_last_error)
    elif st.session_state.eval_last_info:
        st.warning(st.session_state.eval_last_info)

# Auto-refresh functionality - Skip when user is working with evaluation results
# Check if user has active evaluation results to prevent interrupting their analysis
if auto_refresh and not (st.session_state.get('eval_last_df') is not None and st.session_state.get('eval_last_error') is None):
    st.session_state.refresh_counter += 1
    time.sleep(REFRESH_INTERVAL)
    st.rerun()
